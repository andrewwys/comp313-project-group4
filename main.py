# -*- coding: utf-8 -*-
"""COMP313_Team4_20231112_02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yfJiH_vvc4F9EUJQBhvgzVdsci-Nyz4N

# COMP313 - Software Project 2 (Sec.001)
Team 4 - Nov 2023 <br>
Midterm
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import gc
import seaborn as sns

"""### Download and Unzip the data files"""

!mkdir ~/.kaggle #create the .kaggle folder in your root directory
!echo '{"username":"andrewwy428","key":"e5e2e633ebf16e36290be1b41c523403"}' > ~/.kaggle/kaggle.json #write kaggle API credentials to kaggle.json
!chmod 600 ~/.kaggle/kaggle.json  # set permissions
!pip install kaggle #install the kaggle library

!kaggle competitions download -c predict-student-performance-from-game-play -p /content/kaggle/

!unzip /content/kaggle/predict-student-performance-from-game-play.zip -d /content/kaggle/

"""### Data loading and pre-processing"""

local_path = 'C:\\Projects\\COMP313_Project\\Data'
colab_path = '/content/kaggle/'

# Change colab_path/local_path to switch between local and colab
data_path = colab_path

train_file = 'train.csv'
test_file = 'test.csv'
train_label_file = 'train_labels.csv'

def reduce_memory_usage(df):

    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))

    for col in df.columns:
        col_type = df[col].dtype.name
        if ((col_type != 'datetime64[ns]') & (col_type != 'category')):
            if (col_type != 'object'):
                c_min = df[col].min()
                c_max = df[col].max()

                if str(col_type)[:3] == 'int':
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                        df[col] = df[col].astype(np.int64)

                else:
                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                        df[col] = df[col].astype(np.float16)
                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                        df[col] = df[col].astype(np.float32)
                    else:
                        pass
            else:
                df[col] = df[col].astype('category')
    mem_usg = df.memory_usage().sum() / 1024**2
    print("Memory usage became: ",mem_usg," MB")

    return df

import os

dtypes={
    'elapsed_time':np.int32,
    'event_name':'category',
    'name':'category',
    'level':np.uint8,
    'room_coor_x':np.float32,
    'room_coor_y':np.float32,
    'screen_coor_x':np.float32,
    'screen_coor_y':np.float32,
    'hover_duration':np.float32,
    'text':'category',
    'fqid':'category',
    'room_fqid':'category',
    'text_fqid':'category',
    'fullscreen':'category',
    'hq':'category',
    'music':'category',
    'level_group':'category'}

X_full = pd.read_csv(os.path.join(data_path, train_file), dtype=dtypes)
print(f"Shape of the full training dataset: {X_full.shape}")

X_full = reduce_memory_usage(X_full)

# Show the first 5 rows of the data
X_full.head(5)

# Loading the labels and print the shape
y_full = pd.read_csv(os.path.join(data_path, train_label_file))
print(f"Shape of the full training labels: {y_full.shape}")

y_full = reduce_memory_usage(y_full)

# Show the first 5 rows of the labels dataset
y_full.head(5)

# Split the session_id into session and q
y_full['session'] = y_full.session_id.apply(lambda x: int(x.split('_')[0]) )
y_full['q'] = y_full.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )

# Display the modified labels dataset
y_full.head(5)

"""### Data Exploration"""

print(X_full.info())

X_full.describe()

for col in X_full.select_dtypes(include=['category','uint8']).columns:
  print('Number of unique values in ',col,': ', len(X_full[col].unique()))
  print(X_full[col].value_counts(ascending=True))
  print()

X_full.hist(bins=20, figsize=(15, 10))
plt.show()

print(y_full.info())

y_full.describe()

import gc

gc.collect()

for col in y_full.loc[:, ['correct', 'q']]:
  print('Number of unique values in ',col,': ', len(y_full[col].unique()))
  print(y_full[col].value_counts(ascending=True))
  print()

sns.countplot(y_full, x="q", hue="correct")
plt.show()

full_df = pd.concat([X_full, y_full.reindex(X_full.index)], axis=1)
correlation_matrix = full_df.corr()
figsize=(15, 10)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 8})
plt.title(f'correlation matrix of all level')
plt.show()

gc.collect()

# for n in range(1,19):
#   correlation_matrix = full_df.loc[full_df['level'] <= n] .corr()
#   sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 8})
#   plt.title(f'correlation matrix of level {n}')
#   plt.show()

gc.collect()

### Checking for Duplicates

# duplicates = X_full[X_full.duplicated()]

# if duplicates.empty:
#     print("No duplicate rows found.")
# else:
#     print("Duplicate rows:")
#     print(duplicates)

### Finding All the NAN values can thier counts
# Count missing values in each column
missing_values = X_full.isnull().sum()

# Plotting the missing values
plt.figure(figsize=(8, 6))
missing_values.plot(kind='bar', color='skyblue')
plt.title('Missing Values by Column')
plt.xlabel('Columns')
plt.ylabel('Count of Missing Values')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

print(missing_values)

X_full['event_name'].value_counts()

"""### Data Preprocessing

Data preprocessing is one of the most important steps in the Machine Learning (ML)
pipeline (i.e., “Garbage In, Garbage Out”). Discuss your dataset and the method(s) with
respect to data preprocessing and if applicable, cite the sources of your method(s).
Discuss how you partitioned your dataset into training, validation and test sets. Explain
your rationale behind your methodology.

"""

# Select columns for training and put them into Category or Numeric type
CATS = ['event_name', 'fqid', 'room_fqid', 'text']
NUMS = ['elapsed_time','level','page','room_coor_x', 'room_coor_y',
        'screen_coor_x', 'screen_coor_y', 'hover_duration']

# Define EVENTS
EVENTS = X_full['event_name'].unique()
for event in EVENTS:
  print(event)

# Adding more features: number of unique values for CATS data, means and std for NUMS data
def feature_engineer(train):

    dfs = []
    for c in CATS:
        tmp = train.groupby(['session_id','level_group'])[c].agg('nunique')
        tmp.name = tmp.name + '_nunique'
        dfs.append(tmp)
    for c in NUMS:
        tmp = train.groupby(['session_id','level_group'])[c].agg('mean')
        tmp.name = tmp.name + '_mean'
        dfs.append(tmp)
    for c in NUMS:
        tmp = train.groupby(['session_id','level_group'])[c].agg('std')
        tmp.name = tmp.name + '_std'
        dfs.append(tmp)

    df = pd.concat(dfs,axis=1)
    df = df.fillna(-1)
    df = df.reset_index()
    df = df.set_index('session_id')
    return df

# # Handle train data in chunks
# chunks = []
# print(f'Handling train data in 10 chunks to prevent memory issues')
# for i in range(10):
#     print(i,', ',end='')
#     SKIP_ROWS = 0
#     if i>0: SKIP_ROWS = range(1,skips[i]+1)
#     train_data = pd.read_csv('/content/kaggle/train.csv', nrows=reads[i], skiprows=SKIPS)
#     processed_data = feature_engineer(train_data)
#     chunks.append(processed_data)

# # Merge all chunks
# print()
# del train_data; gc.collect()
# df = pd.concat(chunks, axis=0)
# print('Shape of all train data after feature engineering:', df.shape )
# df.head()

X_processed = feature_engineer(X_full)
print('Shape of all train data after feature engineering:', X_processed.shape )
X_processed.head()

X_processed.shape

X_processed.iloc[0]

# Missing values after processing
print(X_processed.isnull().sum())

"""### Train using XGBoost Model

##### Training one model for each of the 18 questions. These questions are divided into different level groups as follows:
- level_group = '0-4' for question 1-3
- level_group = '5-12' for question 4-13
- level_group = '13-22' for question 14-18
"""

# Exclude "level_group" from the features
FEATURES = [c for c in X_processed.columns if c != 'level_group']
print('We will train with', len(FEATURES) ,'features')

# session_id is the index of X_processed, we use it to identify unique users
ALL_USERS = X_processed.index.unique()
print('We will train with', len(ALL_USERS) ,'users info')

from sklearn.model_selection import KFold, GroupKFold
from xgboost import XGBClassifier
from sklearn.metrics import log_loss
from statistics import mean

gkf = GroupKFold(n_splits=5)
oof = pd.DataFrame(data=np.zeros((len(ALL_USERS),18)), index=ALL_USERS)
models = {}
overall_log_loss = []

# COMPUTE CV SCORE WITH 5 GROUP K FOLD
for i, (train_index, test_index) in enumerate(gkf.split(X=X_processed, groups=X_processed.index)):
    print('#'*25)
    print('### Fold',i+1)

    xgb_params = {
    'objective' : 'binary:logistic',
    'eval_metric':'logloss',
    'learning_rate': 0.05,
    'max_depth': 4,
    'n_estimators': 1000,
    'early_stopping_rounds': 50,
    'tree_method':'hist',
    'subsample':0.8,
    'colsample_bytree': 0.4,
    'use_label_encoder' : False}

    split_log_loss = []
    # ITERATE THRU QUESTIONS 1 THRU 18
    for q in range(1,19):

        # USE THIS TRAIN DATA WITH THESE QUESTIONS
        if q<=3: grp = '0-4'
        elif q<=13: grp = '5-12'
        elif q<=22: grp = '13-22'

        # TRAIN DATA
        train_x = X_processed.iloc[train_index]  # get data from kfold split
        train_x = train_x.loc[train_x.level_group == grp] # filter the train data by level_group
        train_users = train_x.index.values
        ### matching the session_id in train_x with the session_id in y_full
        train_y = y_full.loc[y_full.q==q].set_index('session').loc[train_users]

        # VALID DATA
        valid_x = X_processed.iloc[test_index]
        valid_x = valid_x.loc[valid_x.level_group == grp]
        valid_users = valid_x.index.values
        valid_y = y_full.loc[y_full.q==q].set_index('session').loc[valid_users]

        # TRAIN MODEL
        clf =  XGBClassifier(**xgb_params)
        clf.fit(train_x[FEATURES].astype('float32'), train_y['correct'],
                eval_set=[ (valid_x[FEATURES].astype('float32'), valid_y['correct']) ],
                verbose=0)
        log_loss = mean(clf.evals_result()['validation_0']['logloss'])
        print(f'q{q} log_loss = {log_loss}')
        split_log_loss.append(log_loss)

        # SAVE MODEL, PREDICT VALID OOF
        models[f'{grp}_{q}'] = clf
        oof.loc[valid_users, q-1] = clf.predict_proba(valid_x[FEATURES].astype('float32'))[:,1]

    avg_split_log_loss = mean(split_log_loss)
    print(f'Fold {i+1} - Average Log Loss = {avg_split_log_loss}')
    overall_log_loss.append(avg_split_log_loss)

    print('#'*25)
    print()

# Print average log loss for all CVs
print(f'Log loss for 5 CVs: {overall_log_loss}')
print(f'Average: {mean(overall_log_loss)}')

"""### CV Scores:"""

# Put the true labels into dataframe with 18 columns
true = oof.copy()
for k in range(18):
    # GET TRUE LABELS
    tmp = y_full.loc[y_full.q == k+1].set_index('session').loc[ALL_USERS]
    true[k] = tmp.correct.values

true

from sklearn.metrics import f1_score

# Find the best threshold to convert probabilities into 1s and 0s
scores = []; thresholds = []
best_score = 0; best_threshold = 0

for threshold in np.arange(0.4,0.81,0.01):
    print(f'{threshold:.02f}, ',end='')
    preds = (oof.values.reshape((-1))>threshold).astype('int')
    m = f1_score(true.values.reshape((-1)), preds, average='macro')
    scores.append(m)
    thresholds.append(threshold)
    if m>best_score:
        best_score = m
        best_threshold = threshold

# Plot the threshold vs. F1_Score
plt.figure(figsize=(20,5))
plt.plot(thresholds,scores,'-o',color='blue')
plt.scatter([best_threshold], [best_score], color='blue', s=300, alpha=1)
plt.xlabel('Threshold',size=14)
plt.ylabel('Validation F1 Score',size=14)
plt.title(f'Threshold vs. F1_Score with Best F1_Score = {best_score:.3f} at Best Threshold = {best_threshold:.3}',size=18)
plt.show()

# print best score and threshold
print(f'Best F1 score: {best_score:.4f}, best threshold: {best_threshold:.2f}')

print('When using optimal threshold...')
for k in range(18):

    # F1 score for each question
    m = f1_score(true[k].values, (oof[k].values>best_threshold).astype('int'), average='macro')
    print(f'Q{k}: F1 =',m)

# Overall f1 score
m = f1_score(true.values.reshape((-1)), (oof.values.reshape((-1))>best_threshold).astype('int'), average='macro')
print('==> Overall F1 =',m)

# Load the test file and evaluate the model:
X_test = pd.read_csv(os.path.join(data_path, test_file), dtype=dtypes)
print(f"Shape of the full training dataset: {X_test.shape}")

X_test.info()

# Save the models to pickle files
import pickle
for model_name, model in models.items():
    filename = f'{model_name}_model.pkl'
    with open(filename, 'wb') as file:
        pickle.dump(model, file)
        print(f"Model '{model_name}' saved to '{filename}'")